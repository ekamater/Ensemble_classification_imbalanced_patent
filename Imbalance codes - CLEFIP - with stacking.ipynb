{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acc61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow 1.11.0\n",
    "#keras 2.3.1\n",
    "#python 3.6.13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b8c689",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "robust-parks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adadelta, Adam\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler, EarlyStopping\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "from statistics import mean, stdev, median\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27d9b9",
   "metadata": {},
   "source": [
    "### Create some help functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d11b50",
   "metadata": {},
   "source": [
    "### Load csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d06925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patents_with_header(file1):\n",
    "    trainDF = pd.read_csv(file1, header=0, usecols=[0,1])\n",
    "    #trainDF['text']=trainDF['text'].fillna(\"\")\n",
    "    return trainDF\n",
    "\n",
    "def load_patents(file1):\n",
    "    trainDF = pd.read_csv(file1, header=None, usecols=[0,1])\n",
    "    trainDF=trainDF.rename(columns={0: 'label'})\n",
    "    trainDF=trainDF.rename(columns={1: 'text'})\n",
    "    return trainDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-education",
   "metadata": {},
   "source": [
    "### Remove patents based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd182a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove codes used in patents over a threshold\n",
    "def remove_rare_codes1(trainDF, down_threshold, up_threshold):\n",
    "    my_list=Counter(trainDF['label'])\n",
    "    my_list_copy=Counter(trainDF['label'])\n",
    "\n",
    "\n",
    "    for code, freq in my_list.items():\n",
    "        if  freq > down_threshold:\n",
    "            my_list_copy.pop(code, freq)\n",
    "\n",
    "    print(my_list_copy)\n",
    "\n",
    "    labels_copy, texts_copy = [], []\n",
    "\n",
    "    for code, freq in my_list_copy.items():\n",
    "        mm=0\n",
    "        for label in trainDF['label']:\n",
    "            mm=mm+1\n",
    "            if label==code:\n",
    "                labels_copy.append(code)\n",
    "                texts_copy.append(trainDF['text'][mm-1])\n",
    "                \n",
    "    trainDF = pd.DataFrame()\n",
    "    trainDF['text'] = texts_copy\n",
    "    trainDF['label'] = labels_copy\n",
    "    \n",
    "    shuffle(trainDF)\n",
    "    print(trainDF.shape)\n",
    "    \n",
    "    my_list_validation=Counter(trainDF['label'])\n",
    "    print(my_list_validation) \n",
    "    \n",
    "    return trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e580c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove codes used in patents under a threshold\n",
    "def remove_rare_codes_without_up(trainDF, down_threshold, up_threshold):\n",
    "    my_list=Counter(trainDF['label'])\n",
    "    my_list_copy=Counter(trainDF['label'])\n",
    "\n",
    "\n",
    "    for code, freq in my_list.items():\n",
    "        if  freq < down_threshold:\n",
    "            my_list_copy.pop(code, freq)\n",
    "\n",
    "    print(my_list_copy)\n",
    "\n",
    "    labels_copy, texts_copy = [], []\n",
    "\n",
    "    for code, freq in my_list_copy.items():\n",
    "        mm=0\n",
    "        for label in trainDF['label']:\n",
    "            mm=mm+1\n",
    "            if label==code:\n",
    "                labels_copy.append(code)\n",
    "                texts_copy.append(trainDF['text'][mm-1])\n",
    "                \n",
    "    trainDF = pd.DataFrame()\n",
    "    trainDF['text'] = texts_copy\n",
    "    trainDF['label'] = labels_copy\n",
    "    \n",
    "    shuffle(trainDF)\n",
    "    print(trainDF.shape)\n",
    "    \n",
    "    my_list_validation=Counter(trainDF['label'])\n",
    "    print(my_list_validation) \n",
    "    \n",
    "    return trainDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-pleasure",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "saving-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(trainDF):\n",
    "\n",
    "    labels_val=trainDF['label'].values\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(labels_val.reshape(-1, 1))\n",
    "    print(\"Example of an encoded label/target \\n IPC code: \", labels_val[0], \"\\n\", \"One-hot encoding:\", onehot_encoded[0], \"\\n\")\n",
    "    \n",
    "    return onehot_encoded, onehot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd90c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels_new(trainDF, trainDF_test):\n",
    "    #initial    \n",
    "    labels_val=trainDF['label'].values\n",
    "    #new\n",
    "    labels_val_test=trainDF_test['label'].values\n",
    "\n",
    "    labels_all = np.hstack((labels_val, labels_val_test))\n",
    "\n",
    "    onehot_encoder = preprocessing.OneHotEncoder(sparse=False)\n",
    "    onehot_encoded_original = onehot_encoder.fit_transform(labels_all.reshape(-1, 1))\n",
    "\n",
    "    orio=len(labels_val)\n",
    "    orio2=len(labels_all)\n",
    "    \n",
    "    onehot_encoded=onehot_encoded_original[0:orio, :]\n",
    "    onehot_encoded_test=onehot_encoded_original[orio:orio2, :]\n",
    "\n",
    "    return onehot_encoder, onehot_encoded, onehot_encoded_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-arbitration",
   "metadata": {},
   "source": [
    "### Find the number of IPC codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "impossible-honor",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def enumarate_codes(onehot_encoded):\n",
    "\n",
    "    number_of_codes=np.shape(onehot_encoded)\n",
    "    number_of_codes=number_of_codes[1]\n",
    "    print(\"Number of ipc codes: \", number_of_codes, \"\\n\")\n",
    "    \n",
    "    return number_of_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-style",
   "metadata": {},
   "source": [
    "### Split the dataset to train, validation and test data (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vulnerable-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(trainDF, onehot_encoded):\n",
    "\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(trainDF['text'], onehot_encoded, test_size=0.2, random_state=42)#, stratify=onehot_encoded)\n",
    "    test_x, valid_x, test_y, valid_y = train_test_split(valid_x, valid_y, test_size=0.5, random_state=41)\n",
    "    print(\"split_abstract_dataset-Done! \\n\")\n",
    "        \n",
    "    #Number of data per split\n",
    "    number_of_train_data=np.shape(train_x)\n",
    "    number_of_train_data=number_of_train_data[0]\n",
    "    print(\"Number of train data:\", number_of_train_data)\n",
    "\n",
    "    number_of_valid_data=np.shape(valid_x)\n",
    "    number_of_valid_data=number_of_valid_data[0]\n",
    "    print(\"Number of validation data:\",number_of_valid_data)\n",
    "\n",
    "    number_of_test_data=np.shape(test_x)\n",
    "    number_of_test_data=number_of_test_data[0]\n",
    "    print(\"Number of test data:\",number_of_test_data, \"\\n\")\n",
    "    \n",
    "    return train_x, train_y,  valid_x, valid_y, test_x, test_y, number_of_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-release",
   "metadata": {},
   "source": [
    "### Text tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "convertible-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(trainDF):\n",
    "        \n",
    "    token = text.Tokenizer()\n",
    "    token.fit_on_texts(trainDF['text'])\n",
    "    word_index = token.word_index\n",
    "    print('Number of unique words:',len(word_index), \"\\n\")\n",
    "    \n",
    "    return token, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-philadelphia",
   "metadata": {},
   "source": [
    "### Convert text to sequence of tokens and pad them to ensure equal length vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nutritional-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text(number_of_words, token, train_x, valid_x, test_x):\n",
    "    \n",
    "    maxlen=number_of_words\n",
    "\n",
    "    train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen)\n",
    "    valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen)\n",
    "    test_seq_x = sequence.pad_sequences(token.texts_to_sequences(test_x), maxlen)\n",
    "    print('convert text to tokens - Done! \\n')\n",
    "\n",
    "    return train_seq_x, valid_seq_x, test_seq_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-quest",
   "metadata": {},
   "source": [
    "### Text representation - Load a pre-trained word embedding\n",
    "\n",
    "We used the FastText and Patent-300 [1] \n",
    "The original function is provided by [fasttext.cc](https://fasttext.cc/docs/en/english-vectors.html) and corrected based on https://github.com/facebookresearch/fastText/issues/882\n",
    "\n",
    "[1] Risch, J., & Krestel, R. (2019). Domain-specific word embeddings for patent classification. Data Technologies and Applications, 53(1), 108-122. (The Patent-300 embedding was provided by the authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rural-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(fname):\n",
    "    data = {}\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())    \n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(list(map(float, tokens[1:])))\n",
    "    print(\"load_patentVec-Done! \\n\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-stable",
   "metadata": {},
   "source": [
    "### Create a token-embedding mapping / an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "speaking-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embeddings_index, word_index):\n",
    "    \n",
    "    num_words=len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, 300))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-allocation",
   "metadata": {},
   "source": [
    "### Kill the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "light-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_model():\n",
    "    try:\n",
    "        K.clear_session()\n",
    "        del model\n",
    "    except:\n",
    "        print('No model to clear \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-mechanism",
   "metadata": {},
   "source": [
    "### Define a Bi-LSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "through-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_lstm(maxlen, num_words, number_of_codes, embedding_matrix):\n",
    "    \n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((maxlen, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(num_words, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.1)(embedding_layer)\n",
    "\n",
    "    # Add a bi-directional layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100, recurrent_dropout=0.1, dropout=0.1))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    # output_layer1 = layers.Dropout(0.25)(lstm_layer)\n",
    "    output_layer2 = layers.Dense(number_of_codes, activation=\"softmax\")(lstm_layer)\n",
    " \n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-petersburg",
   "metadata": {},
   "source": [
    "### Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "egyptian-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(test_seq_x, test_y, classifier):\n",
    "    \n",
    "    predictions = classifier.predict(test_seq_x)\n",
    "    prediction = np.argmax(predictions, axis = -1) \n",
    "    y_true = np.argmax(test_y,axis = -1)\n",
    "    print('make_predictions-Done! \\n')\n",
    "\n",
    "    return predictions, prediction, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-father",
   "metadata": {},
   "source": [
    "### Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "apart-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, prediction, y_true, number_of_test_data):\n",
    "    \n",
    "    #Accuracy \n",
    "    accuracy_total=metrics.accuracy_score(prediction, y_true)*100\n",
    "    print(\"Accuracy:\", accuracy_total)\n",
    "\n",
    "    #MRR, P@3, P@5, P@10\n",
    "    all_rr=[]\n",
    "    number_of_top_three=0\n",
    "    number_of_top_five=0\n",
    "    number_of_top_ten=0\n",
    "\n",
    "    predictions_2=predictions.argsort()\n",
    "    predictions_3=np.fliplr(predictions_2)\n",
    "    for i in range (0, number_of_test_data):\n",
    "        specific_prediction=predictions_3[i,:]\n",
    "        list1 = specific_prediction.tolist()\n",
    "        target=y_true[i]\n",
    "        prediction_rank=list1.index(target)+1 \n",
    "        #MRR\n",
    "        RR=1/prediction_rank\n",
    "        all_rr.append(RR)\n",
    "        #P@3\n",
    "        if prediction_rank<= 3:\n",
    "            number_of_top_three=number_of_top_three+1\n",
    "        #P@5\n",
    "        if prediction_rank<= 5:\n",
    "            number_of_top_five=number_of_top_five+1     \n",
    "        #P@10\n",
    "        if prediction_rank<= 10:\n",
    "            number_of_top_ten=number_of_top_ten+1 \n",
    "    MRR=np.mean(all_rr)\n",
    "    print(\"MRR:\", MRR)\n",
    "    P3=number_of_top_three/number_of_test_data*100\n",
    "    print(\"P@3:\", P3)\n",
    "    P5=number_of_top_five/number_of_test_data*100\n",
    "    print(\"P@5:\", P5)\n",
    "    P10=number_of_top_ten/number_of_test_data*100\n",
    "    print(\"P@10:\", P10)\n",
    "    \n",
    "    return accuracy_total, MRR, P3, P5, P10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-comparative",
   "metadata": {},
   "source": [
    "# Main code - Loop for different number of words and removing codes thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eed3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words = [61, 101, 201, 301, 401, 501]\n",
    "words = [60]\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "number_of_dataset = [1,2]\n",
    "\n",
    "for i, number_of_words in enumerate(words):\n",
    "    for i, dataset in enumerate(number_of_dataset):        \n",
    "        if dataset==1:            \n",
    "            \n",
    "            #load the file and remove the codes used in less than 500 patents\n",
    "            trainDF = load_patents(\"abstract.csv\")  #https://github.com/ekamater/CLEFIP-0.54M \n",
    "            trainDF_p1=remove_rare_codes_without_up(trainDF, 500, 0)\n",
    "\n",
    "            #load the file with the removed codes\n",
    "            #trainDF_p1 = load_patents_with_header('500_clefip.csv')\n",
    "            \n",
    "            #suffle and reset the index\n",
    "            idx = np.random.permutation(trainDF_p1.index)\n",
    "            trainDF_p1=trainDF_p1.reindex(idx)\n",
    "            trainDF_p1=trainDF_p1.reset_index(drop=True)\n",
    "            \n",
    "            #store the file with the removed codes\n",
    "            #trainDF_p1.to_csv('500_clefip.csv', index=False)  \n",
    "            \n",
    "            #encode the labels\n",
    "            onehot_encoder, onehot_encoded_initial, onehot_encoded =encode_labels_new(trainDF, trainDF_p1) \n",
    "            number_of_codes=enumarate_codes(onehot_encoded)\n",
    "            \n",
    "            #split, tokenize, convert and pad the text of patents with the removed codes\n",
    "            train_x_p1, train_y_p1, valid_x_p1, valid_y_p1, test_x_p1, test_y_p1, number_of_test_data_p1=split_dataset(trainDF_p1, onehot_encoded)\n",
    "            token_p1, word_index_p1=tokenize_text(trainDF)\n",
    "            train_seq_x_p1, valid_seq_x_p1, test_seq_x_p1 =convert_text(number_of_words, token_p1, train_x_p1, valid_x_p1, test_x_p1)\n",
    "\n",
    "            #split, convert and pad the text of all patents \n",
    "            train_x, train_y, valid_x, valid_y, test_x, test_y, number_of_test_data=split_dataset(trainDF, onehot_encoded_initial)\n",
    "            train_seq_x, valid_seq_x, test_seq_x =convert_text(number_of_words, token_p1, train_x, valid_x, test_x)     \n",
    "    \n",
    "            #create the embedding matrix\n",
    "            #embeddings_index = load_fasttext('patent-300.vec')\n",
    "            embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "            \n",
    "            #create the classifier\n",
    "            kill_model()\n",
    "            classifier1 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "            \n",
    "            #train the classifier\n",
    "            earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)            \n",
    "            filepath=\"weights.best.hdf5\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            \n",
    "            history1=classifier1.fit(train_seq_x_p1, train_y_p1, \\\n",
    "                                     validation_data = (valid_seq_x_p1, valid_y_p1), \\\n",
    "                                     callbacks=[earlystop, checkpoint], \\\n",
    "                                     epochs=epochs, \\\n",
    "                                     batch_size=batch_size, \\\n",
    "                                     verbose=1)\n",
    "            \n",
    "            #save the trained classifier\n",
    "            #classifier1.save(\"abstract_500_clefip\")\n",
    "            \n",
    "            #predictions for testing patents with the removed codes\n",
    "            predictions_p1, prediction_p1, y_true1=make_predictions(test_seq_x_p1, test_y_p1, classifier1)\n",
    "            accuracy_total1, MRR1, P3_1, P5_1, P10_1 = calculate_metrics(predictions_p1, prediction_p1, y_true1, number_of_test_data_p1)\n",
    "\n",
    "            #predictions for all patents \n",
    "            predictions, prediction, y_true=make_predictions(test_seq_x, test_y, classifier1)\n",
    "            accuracy_total, MRR, P3, P5, P10 = calculate_metrics(predictions, prediction, y_true, number_of_test_data)\n",
    "        \n",
    "            #store the results\n",
    "            #df=pd.DataFrame(predictions)\n",
    "            #df.sort_values(by=0, axis=1, ascending=False)\n",
    "            #df.to_csv('predictions_500_all.csv', header=False, index=False)          \n",
    "            \n",
    "            #df_p1=pd.DataFrame(predictions_p1)\n",
    "            #df_p1.sort_values(by=0, axis=1, ascending=False)\n",
    "            #df_p1.to_csv('predictions_500_part.csv', header=False, index=False)          \n",
    "            \n",
    "            #df_q_rel=pd.DataFrame(y_true)\n",
    "            #df_q_rel.to_csv('qrel_500_all.csv', header=False, index=False, sep=',') \n",
    "            \n",
    "        if dataset==2:            \n",
    "\n",
    "            #load the file and remove the codes used in more than 500 patents\n",
    "            trainDF = load_patents(\"abstract.csv\")\n",
    "            trainDF_p2=remove_rare_codes1(trainDF, 500, 0)\n",
    "\n",
    "            #load the file with the removed codes\n",
    "            #trainDF_p2 = load_patents_with_header('0-500_clefip.csv')\n",
    "            \n",
    "            #suffle and reset the index\n",
    "            idx = np.random.permutation(trainDF_p2.index)\n",
    "            trainDF_p2=trainDF_p2.reindex(idx)\n",
    "            trainDF_p2=trainDF_p2.reset_index(drop=True)\n",
    "            \n",
    "            #store the file with the removed codes\n",
    "            #trainDF_p2.to_csv('0-500_clefip.csv', index=False)  \n",
    "            \n",
    "            #encode the labels\n",
    "            onehot_encoder, onehot_encoded_initial, onehot_encoded =encode_labels_new(trainDF, trainDF_p2) \n",
    "            number_of_codes=enumarate_codes(onehot_encoded)\n",
    "            \n",
    "            #split, tokenize (use the same tokenizer), convert and pad the text of patents with the removed codes\n",
    "            train_x_p2, train_y_p2, valid_x_p2, valid_y_p2, test_x_p2, test_y_p2, number_of_test_data_p2=split_dataset(trainDF_p2, onehot_encoded)\n",
    "            train_seq_x_p2, valid_seq_x_p2, test_seq_x_p2 =convert_text(number_of_words, token_p1, train_x_p2, valid_x_p2, test_x_p2)\n",
    "\n",
    "            #split, convert and pad the text of all patents \n",
    "            train_x, train_y, valid_x, valid_y, test_x, test_y, number_of_test_data=split_dataset(trainDF, onehot_encoded_initial)\n",
    "            train_seq_x, valid_seq_x, test_seq_x =convert_text(number_of_words, token_p1, train_x, valid_x, test_x)     \n",
    "    \n",
    "            #create the embedding matrix\n",
    "            #embeddings_index = load_fasttext('patent-300.vec')\n",
    "            #embedding_matrix_p1, num_words_p1 =create_embedding_matrix(embeddings_index, word_index_p1)\n",
    "            \n",
    "            #create the classifier\n",
    "            kill_model()\n",
    "            classifier2 = create_bidirectional_lstm(number_of_words, num_words_p1, number_of_codes, embedding_matrix_p1) \n",
    "            \n",
    "            #train the classifier\n",
    "            earlystop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)            \n",
    "            filepath=\"weights.best.hdf5\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            \n",
    "            history2=classifier2.fit(train_seq_x_p2, train_y_p2, \\\n",
    "                                     validation_data = (valid_seq_x_p2, valid_y_p2), \\\n",
    "                                     callbacks=[earlystop, checkpoint], \\\n",
    "                                     epochs=epochs, \\\n",
    "                                     batch_size=batch_size, \\\n",
    "                                     verbose=1)\n",
    "            \n",
    "            #save the trained classifier\n",
    "            #classifier2.save(\"abstract_0-500_clefip\")\n",
    "            \n",
    "            #predictions for testing patents with the removed codes\n",
    "            predictions_p2, prediction_p2, y_true2=make_predictions(test_seq_x_p2, test_y_p2, classifier2)\n",
    "            accuracy_total2, MRR2, P3_2, P5_2, P10_2 = calculate_metrics(predictions_p2, prediction_p2, y_true2, number_of_test_data_p2)\n",
    "\n",
    "            #predictions for all patents \n",
    "            predictions_, prediction_, y_true_=make_predictions(test_seq_x, test_y, classifier2)\n",
    "            accuracy_total_, MRR_, P3_, P5_, P10_ = calculate_metrics(predictions_, prediction_, y_true_, number_of_test_data)\n",
    "        \n",
    "            #store the results\n",
    "            #df=pd.DataFrame(predictions_)\n",
    "            #df.sort_values(by=0, axis=1, ascending=False)\n",
    "            #df.to_csv('predictions_0-500_all.csv', header=False, index=False)          \n",
    "            \n",
    "            #df_p1=pd.DataFrame(predictions_p2)\n",
    "            #df_p1.sort_values(by=0, axis=1, ascending=False)\n",
    "            #df_p1.to_csv('predictions_0-500_part.csv', header=False, index=False)          \n",
    "            \n",
    "            #df_q_rel=pd.DataFrame(y_true_)\n",
    "            #df_q_rel.to_csv('qrel_0-500_all.csv', header=False, index=False, sep=',') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9656a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_predictions-Done! \n",
      "\n",
      "Accuracy: 2.0764864057364805\n",
      "MRR: 0.07450322711977107\n",
      "P@3: 6.76725425754407\n",
      "P@5: 10.531819539886465\n",
      "P@10: 16.821033761577535\n",
      "make_predictions-Done! \n",
      "\n",
      "Accuracy: 0.20512630975920756\n",
      "MRR: 0.010233329694180957\n",
      "P@3: 0.7484338329052168\n",
      "P@5: 1.2307578585552454\n",
      "P@10: 2.0457191432742596\n"
     ]
    }
   ],
   "source": [
    "            #predictions for testing patents with the removed codes\n",
    "            predictions_p2, prediction_p2, y_true2=make_predictions(test_seq_x_p2, test_y_p2, classifier2)\n",
    "            accuracy_total2, MRR2, P3_2, P5_2, P10_2 = calculate_metrics(predictions_p2, prediction_p2, y_true2, number_of_test_data_p2)\n",
    "\n",
    "            #predictions for all patents \n",
    "            predictions_, prediction_, y_true_=make_predictions(test_seq_x, test_y, classifier2)\n",
    "            accuracy_total_, MRR_, P3_, P5_, P10_ = calculate_metrics(predictions_, prediction_, y_true_, number_of_test_data)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fe7d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' stacking with probabilities\n",
    "meta_x_train contains all probalilities for all labels and\n",
    "meta_y_train contains one hot encoding for all labels'''\n",
    "\n",
    "n_learners = 2\n",
    "num_classes = number_of_codes # from the dataset\n",
    "       \n",
    "n_trains = train_seq_x.shape[0]\n",
    "n_tests = test_seq_x.shape[0]\n",
    "    \n",
    "test_accuracy_records = []\n",
    "    \n",
    "meta_x_train = np.zeros((n_trains, n_learners*num_classes), dtype=\"float32\")\n",
    "meta_x_test = np.zeros((n_tests, n_learners*num_classes), dtype=\"float32\")\n",
    "\n",
    "for i in range(n_learners):\n",
    "            \n",
    "    if i==0:\n",
    "        meta_x_train[:, i*num_classes:i*num_classes + num_classes] = classifier1.predict(train_seq_x, verbose=0)\n",
    "        meta_x_test[:, i*num_classes:i*num_classes + num_classes] = classifier1.predict(test_seq_x, verbose=0)\n",
    "                 \n",
    "    elif i==1:\n",
    "        meta_x_train[:, i*num_classes:i*num_classes + num_classes] = classifier2.predict(train_seq_x, verbose=0)\n",
    "        meta_x_test[:, i*num_classes:i*num_classes + num_classes] = classifier2.predict(test_seq_x, verbose=0)                   \n",
    "\n",
    "    else:\n",
    "        break\n",
    "           \n",
    "    # construct meta learning problem        \n",
    "    meta_y_train = train_y # use one hot encode\n",
    "    meta_y_test = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model to clear \n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1462)              2138906   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 731)               1069453   \n",
      "=================================================================\n",
      "Total params: 3,208,359\n",
      "Trainable params: 3,208,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432904 samples, validate on 54113 samples\n",
      "Epoch 1/20\n",
      "103296/432904 [======>.......................] - ETA: 3:50 - loss: 3.3929 - accuracy: 0.2960"
     ]
    }
   ],
   "source": [
    "#meta_model_softmax\n",
    "\n",
    "def meta_model_softmax(n_learners, num_classes):\n",
    "    '''create a feedforward model to train the meta model'''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    in_dim = n_learners * num_classes\n",
    "    model.add(Dense(n_learners*num_classes, input_dim = in_dim, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "meta_epochs = 20\n",
    "\n",
    "kill_model()\n",
    "super_model = meta_model_softmax(n_learners, num_classes)     \n",
    "super_model.fit(meta_x_train, meta_y_train, \\\n",
    "                batch_size=128, \\\n",
    "                epochs=meta_epochs, \\\n",
    "                validation_data=(meta_x_test, meta_y_test), \\\n",
    "                shuffle=True)\n",
    "    \n",
    "scores_softmax = super_model.evaluate(meta_x_test, meta_y_test, verbose=1)\n",
    "print('Stack test accuracy: ', scores_softmax[1]) \n",
    "    \n",
    "probs_softmax=super_model.predict(meta_x_test)\n",
    "predict_softmax = np.argmax(probs_softmax, axis=-1)\n",
    "y_true= np.argmax(meta_y_test, axis=-1)\n",
    "accuracy_total, MRR, P3, P5, P10 = calculate_metrics(probs_softmax, predict_softmax, y_true, number_of_test_data)\n",
    "\n",
    "#store the predictions\n",
    "df=pd.DataFrame(probs_softmax)\n",
    "df.sort_values(by=0, axis=1, ascending=False)\n",
    "df.to_csv('predictions_stack_abstract_clefip_softmax.csv', header=False, index=False)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_model_sigmoid\n",
    "\n",
    "def meta_model_sigmoid(n_learners, num_classes):\n",
    "    '''create a feedforward model to train the meta model'''\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    in_dim = n_learners * num_classes\n",
    "    model.add(Dense(n_learners*num_classes, input_dim = in_dim, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "meta_epochs = 20\n",
    "\n",
    "kill_model()\n",
    "super_model = meta_model_sigmoid(n_learners, num_classes) \n",
    "super_model.fit(meta_x_train, meta_y_train, batch_size=128, epochs=meta_epochs, validation_data=(meta_x_test, meta_y_test), shuffle=True)\n",
    "\n",
    "scores_sigmoid = super_model.evaluate(meta_x_test, meta_y_test, verbose=1)\n",
    "print('Stack test accuracy: ', scores_sigmoid[1])\n",
    "       \n",
    "probs_sigmoid=super_model.predict(meta_x_test)\n",
    "predict_sigmoid = np.argmax(probs_sigmoid, axis=-1)\n",
    "y_true= np.argmax(meta_y_test, axis=-1)\n",
    "\n",
    "accuracy_total, MRR, P3, P5, P10 = calculate_metrics(probs_sigmoid, predict_sigmoid, y_true, number_of_test_data) \n",
    "\n",
    "#store the predictions   \n",
    "df=pd.DataFrame(probs_sigmoid)\n",
    "df.sort_values(by=0, axis=1, ascending=False)\n",
    "df.to_csv('predictions_stack_title_clefip_sigmoid.csv', header=False, index=False)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e432c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
